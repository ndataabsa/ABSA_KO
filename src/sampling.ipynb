{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf1d49b-15a1-43e3-a987-6d37de59f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def format_line(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r'\\s*,\\s*', ' , ', line)\n",
    "    line = re.sub(r'\\s*\\.\\s*', ' . ', line)\n",
    "    line = re.sub(r\"n \\s*'\\s*t\", \"n't\", line)\n",
    "    return line\n",
    "\n",
    "def split_read(file_name):\n",
    "    inputs, targets =[], []\n",
    "    with open(file_name, 'r', encoding='UTF-8') as fp:\n",
    "        for line in fp:\n",
    "            input, target = [], []\n",
    "            input, target = line.strip().split('####')\n",
    "            if line != '':\n",
    "                inputs.append(input)\n",
    "                targets.append(target)\n",
    "    print('Data read. Total count: ',len(targets))\n",
    "    return inputs, targets\n",
    "\n",
    "def merge_save(inputs, targets, file_name, num=1):\n",
    "    # 각 target 항목을 num 번씩 복사하여 확장\n",
    "    targets_expanded = [tar for tar in targets for _ in range(num)]\n",
    "\n",
    "    # 병합\n",
    "    merged_data = [inp + \"####\" + tar for inp, tar in zip(inputs, targets_expanded)]\n",
    "    # 파일로 저장\n",
    "    with open(file_name, 'w', encoding='UTF-8') as file:\n",
    "        for line in merged_data:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "    # 정보 출력\n",
    "    print('Input count:', len(inputs))\n",
    "    print('Expanded target count:', len(targets_expanded))\n",
    "    print('Merged data count:', len(merged_data))\n",
    "    print('Data saved. Total count:', len(merged_data))\n",
    "\n",
    "def dev_sampleing(dir, file_path1, file_path2, formatted=True):\n",
    "    seed = 2024  # seed 값\n",
    "    train_ratio = 0.9  # train 비율 (90%)\n",
    "    inputs, targets = [], []\n",
    "    with open(file_path1, 'r', encoding='UTF-8') as file:\n",
    "        for line in file:\n",
    "            if formatted == True:\n",
    "                formatted_line = format_line(line.strip())\n",
    "            else:\n",
    "                formatted_line = line.strip()\n",
    "            inputs.append(formatted_line)\n",
    "\n",
    "    with open(file_path2, 'r', encoding='UTF-8') as file:\n",
    "        for line in file:\n",
    "            input, target = line.strip().split('####')\n",
    "            if line != '':\n",
    "                targets.append(target)\n",
    "    lines = [inp + \"####\" + tar for inp, tar in zip(inputs, targets)]\n",
    "    # Shuffle the data\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Split the data\n",
    "    split_index = int(len(lines) * train_ratio)\n",
    "    train_data = lines[:split_index]\n",
    "    train1, train2, train3 = zip(*[s.split('####') for s in train_data])\n",
    "    train_input = [inp + \"####\" + tar for inp, tar in zip(train1, train2)]\n",
    "    train_quad = [inp + \"####\" + tar for inp, tar in zip(train1, train3)]\n",
    "    dev_data = lines[split_index:]\n",
    "    dev1, dev2, dev3 = zip(*[s.split('####') for s in train_data])\n",
    "    dev_input = [inp + \"####\" + tar for inp, tar in zip(dev1, dev2)]\n",
    "    dev_quad = [inp + \"####\" + tar for inp, tar in zip(dev1, dev3)]\n",
    "\n",
    "    train_file = os.path.join(dir,f'train.txt')\n",
    "    dev_file = os.path.join(dir,f'dev.txt')\n",
    "    train_file_target = os.path.join(dir,f'train_quad.txt')\n",
    "   \n",
    "    # Save the data to the output files\n",
    "    with open(train_file, 'w') as file:\n",
    "        file.writelines(line + '\\n' for line in train_input)\n",
    "    \n",
    "    with open(dev_file, 'w') as file:\n",
    "        file.writelines(line + '\\n' for line in dev_input)\n",
    "\n",
    "    with open(train_file_target, 'w') as file:\n",
    "        file.writelines(line + '\\n' for line in train_quad)\n",
    "\n",
    "    \n",
    "    print(f\"Train data count: {len(train_data)}, Dev data count: {len(dev_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6491ece7-b4bb-4c62-b78d-09ae577616f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data count: 5905, Dev data count: 657\n"
     ]
    }
   ],
   "source": [
    "dir = '/home/elicer/SS/data/split/attempt01'\n",
    "file_path1 = '/home/elicer/SS/data/split/attempt01/zero_train_1.txt'\n",
    "file_path2 = '/home/elicer/SS/data/split/train_org.txt'\n",
    "dev_sampleing(dir, file_path1, file_path2, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cd4d90-383a-49c0-8915-e8786cde81fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6060"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/elicer/ATOSS/outputs/asqp/post_data1.0/result_final1_10_train_asqp_rest16_only_n_beam10.pickle'\n",
    "with open(file_path, 'rb') as f:\n",
    "    loaded_object = pd.read_pickle(f)\n",
    "len(loaded_object[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2daf6bc2-0d74-4789-9129-d10677661a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read. Total count:  606\n",
      "Input count: 6060\n",
      "Expanded target count: 6060\n",
      "Merged data count: 6060\n",
      "Data saved. Total count: 6060\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/elicer/ATOSS/data/asqp'\n",
    "file_name = os.path.join(file_path, 'train_asqp_rest16_only.txt')\n",
    "inputs, targets = split_read(file_name)\n",
    "file_name = os.path.join(file_path, 'train_asqp_rest16_only_n10.txt')\n",
    "merge_save(loaded_object[0], targets, file_name, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04dcf92e-ea12-4c10-8262-f1dd2f6bc8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29525"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/elicer/SS/outputs/split/attempt01/result_split_attempt01_train_beam5.pickle'\n",
    "with open(file_path, 'rb') as f:\n",
    "    loaded_object = pd.read_pickle(f)\n",
    "len(loaded_object[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb2a878-9274-4926-84d1-8e871a605048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read. Total count:  606\n",
      "Input count: 606\n",
      "Expanded target count: 606\n",
      "Merged data count: 606\n",
      "Data saved. Total count: 606\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/elicer/ATOSS/data/asqp'\n",
    "file_name = os.path.join(file_path, 'train_asqp_rest16_only.txt')\n",
    "inputs, targets = split_read(file_name)\n",
    "file_name = os.path.join(file_path, 'train_asqp_rest16_only_n.txt')\n",
    "merge_save(inputs, inputs, file_name, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70188b9a-5166-49c0-8b01-dbcd8d5f4cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/elicer/SS/outputs/split/attempt01/final/result_split_attempt01_test-dpo_beam1.pickle'\n",
    "with open(file_path, 'rb') as f:\n",
    "    loaded_object = pd.read_pickle(f)\n",
    "len(loaded_object[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29687d0b-cc58-4231-8aee-c11be1afbd2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read. Total count:  583\n",
      "Input count: 583\n",
      "Expanded target count: 583\n",
      "Merged data count: 583\n",
      "Data saved. Total count: 583\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/elicer/SS/data/split/attempt01'\n",
    "file_name = os.path.join(file_path, 'test_org.txt')\n",
    "inputs, targets = split_read(file_name)\n",
    "file_name = os.path.join(file_path, 'test_SS.txt')\n",
    "merge_save(loaded_object[0], targets, file_name, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20914c0a-062a-4912-9ebe-81a9d7dfe58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvp_seo",
   "language": "python",
   "name": "mvp_seo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
